{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Absolute Error: 806130.940497786\n",
      "Linear Regression - Mean Squared Error: 1200394629952.1387\n",
      "Linear Regression - R^2: 0.6318615631007681\n",
      "\n",
      "Decision Tree - Mean Absolute Error: 1000125.1070078366\n",
      "Decision Tree - Mean Squared Error: 2008150667497.0679\n",
      "Decision Tree - R^2: 0.3573342600434061\n",
      "\n",
      "Random Forest - Mean Absolute Error: 821905.4477716297\n",
      "Random Forest - Mean Squared Error: 1312101748375.1194\n",
      "Random Forest - R^2: 0.6083633997508636\n",
      "\n",
      "Gradient Boosting - Mean Absolute Error: 826092.7293674946\n",
      "Gradient Boosting - Mean Squared Error: 1364167975030.7717\n",
      "Gradient Boosting - R^2: 0.585185711602817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Lade dein Dataset\n",
    "data = pd.read_csv('Housing.csv')\n",
    "\n",
    "# Kodieren der kategorischen Merkmale in numerische Werte\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Splitte das Dataset in Features (X) und Target (y)\n",
    "X = data.drop('price', axis=1)  # Features (alle Spalten außer 'price')\n",
    "y = data['price']  # Zielvariable (Preis)\n",
    "\n",
    "# Definiere die Regressionsmodelle, die du testen möchtest\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# 5-fache Kreuzvalidierung\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results = {model_name: {'mse': [], 'mae': [], 'r2': []} for model_name in models.keys()}\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r_squared = 1 - mse / np.var(y_test)\n",
    "        \n",
    "        results[model_name]['mse'].append(mse)\n",
    "        results[model_name]['mae'].append(mae)\n",
    "        results[model_name]['r2'].append(r_squared)\n",
    "\n",
    "# Durchschnittliche Fehlermaße über die 5 Folds berechnen\n",
    "for model_name in models.keys():\n",
    "    avg_mse = np.mean(results[model_name]['mse'])\n",
    "    avg_mae = np.mean(results[model_name]['mae'])\n",
    "    avg_r2 = np.mean(results[model_name]['r2'])\n",
    "    \n",
    "    print(f\"{model_name} - Mean Absolute Error: {avg_mae}\")\n",
    "    print(f\"{model_name} - Mean Squared Error: {avg_mse}\")\n",
    "    print(f\"{model_name} - R^2: {avg_r2}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Absolute Error: 808096.4059761824\n",
      "Linear Regression - Mean Squared Error: 1212669771709.4429\n",
      "Linear Regression - R^2: 0.6477055410558171\n",
      "\n",
      "Lasso Regression - Mean Absolute Error: 808095.3086202976\n",
      "Lasso Regression - Mean Squared Error: 1212670138510.1226\n",
      "Lasso Regression - R^2: 0.6477055505104865\n",
      "\n",
      "Ridge Regression - Mean Absolute Error: 805920.2556641481\n",
      "Ridge Regression - Mean Squared Error: 1212675827805.236\n",
      "Ridge Regression - R^2: 0.6481755557505029\n",
      "\n",
      "Decision Tree - Mean Absolute Error: 1032638.5670892993\n",
      "Decision Tree - Mean Squared Error: 2112409826045.3188\n",
      "Decision Tree - R^2: 0.38296932218121615\n",
      "\n",
      "Random Forest - Mean Absolute Error: 877362.5366530225\n",
      "Random Forest - Mean Squared Error: 1504393171156.2156\n",
      "Random Forest - R^2: 0.5691086780831913\n",
      "\n",
      "Gradient Boosting - Mean Absolute Error: 813776.2998545977\n",
      "Gradient Boosting - Mean Squared Error: 1366947096900.139\n",
      "Gradient Boosting - R^2: 0.602941708376014\n",
      "\n",
      "XGBoost - Mean Absolute Error: 851684.425\n",
      "XGBoost - Mean Squared Error: 1445698129100.8\n",
      "XGBoost - R^2: 0.5776937221239182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Lade dein Dataset\n",
    "data = pd.read_csv('Housing.csv')\n",
    "\n",
    "# Kodieren der kategorischen Merkmale in numerische Werte\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Erstelle eine Preis-Kategorie für gleichmäßige Verteilung in den Folds\n",
    "data['price_category'] = pd.qcut(data['price'], q=5, labels=False)\n",
    "\n",
    "# Splitte das Dataset in Features (X) und Target (y)\n",
    "X = data.drop(['price', 'price_category'], axis=1)  # Features (alle Spalten außer 'price' und 'price_category')\n",
    "y = data['price']  # Zielvariable (Preis)\n",
    "\n",
    "# Wende den MinMaxScaler an\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)  # Umwandlung zurück in DataFrame\n",
    "\n",
    "# Definiere die Regressionsmodelle, die du testen möchtest\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso(alpha=1.0)  # L1-Regularisierung\n",
    "ridge = Ridge(alpha=1.0)  # L2-Regularisierung\n",
    "dt = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "xgb = XGBRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': lr,\n",
    "    'Lasso Regression': lasso,\n",
    "    'Ridge Regression': ridge,\n",
    "    'Decision Tree': dt,\n",
    "    'Random Forest': rf,\n",
    "    'Gradient Boosting': gb,\n",
    "    'XGBoost': xgb,\n",
    "}\n",
    "\n",
    "# Erstelle den Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('Linear Regression', lr),\n",
    "    ('Lasso Regression', lasso),\n",
    "    ('Ridge Regression', ridge),\n",
    "    ('Decision Tree', dt),\n",
    "    ('Random Forest', rf),\n",
    "    ('Gradient Boosting', gb),\n",
    "    ('XGBoost', xgb)\n",
    "])\n",
    "\n",
    "# 5-fache stratifizierte Kreuzvalidierung\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results = {model_name: {'mse': [], 'mae': [], 'r2': []} for model_name in models.keys()}\n",
    "\n",
    "for train_index, test_index in kf.split(X, data['price_category']):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r_squared = 1 - mse / np.var(y_test)\n",
    "        \n",
    "        results[model_name]['mse'].append(mse)\n",
    "        results[model_name]['mae'].append(mae)\n",
    "        results[model_name]['r2'].append(r_squared)\n",
    "\n",
    "# Durchschnittliche Fehlermaße über die 5 Folds berechnen\n",
    "for model_name in models.keys():\n",
    "    avg_mse = np.mean(results[model_name]['mse'])\n",
    "    avg_mae = np.mean(results[model_name]['mae'])\n",
    "    avg_r2 = np.mean(results[model_name]['r2'])\n",
    "    \n",
    "    print(f\"{model_name} - Mean Absolute Error: {avg_mae}\")\n",
    "    print(f\"{model_name} - Mean Squared Error: {avg_mse}\")\n",
    "    print(f\"{model_name} - R^2: {avg_r2}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Absolute Error: 797403.6997686285\n",
      "Linear Regression - Mean Squared Error: 1183730297226.8325\n",
      "Linear Regression - R^2: 0.6610286589435679\n",
      "\n",
      "Lasso Regression - Mean Absolute Error: 797402.5796475348\n",
      "Lasso Regression - Mean Squared Error: 1183730395453.0757\n",
      "Lasso Regression - R^2: 0.6610286308156392\n",
      "\n",
      "Ridge Regression - Mean Absolute Error: 796105.3804100065\n",
      "Ridge Regression - Mean Squared Error: 1184196397107.799\n",
      "Ridge Regression - R^2: 0.6608951872379879\n",
      "\n",
      "Decision Tree - Mean Absolute Error: 1104328.495696463\n",
      "Decision Tree - Mean Squared Error: 2481925356076.1963\n",
      "Decision Tree - R^2: 0.28927934993126514\n",
      "\n",
      "Random Forest - Mean Absolute Error: 837203.650462113\n",
      "Random Forest - Mean Squared Error: 1388401822169.2297\n",
      "Random Forest - R^2: 0.6024192092671312\n",
      "\n",
      "Gradient Boosting - Mean Absolute Error: 803751.3722534117\n",
      "Gradient Boosting - Mean Squared Error: 1341249133114.1177\n",
      "Gradient Boosting - R^2: 0.615921786907387\n",
      "\n",
      "XGBoost - Mean Absolute Error: 846269.0211009175\n",
      "XGBoost - Mean Squared Error: 1403108067768.0444\n",
      "XGBoost - R^2: 0.5982079477082275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Lade dein Dataset\n",
    "data = pd.read_csv('Housing.csv')\n",
    "\n",
    "# Kodieren der kategorischen Merkmale in numerische Werte\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Splitte das Dataset in Features (X) und Target (y)\n",
    "X = data.drop(['price'], axis=1)  # Features (alle Spalten außer 'price')\n",
    "y = data['price']  # Zielvariable (Preis)\n",
    "\n",
    "# Wende den MinMaxScaler an\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)  # Umwandlung zurück in DataFrame\n",
    "\n",
    "# Definiere die Regressionsmodelle, die du testen möchtest\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso(alpha=1.0)  # L1-Regularisierung\n",
    "ridge = Ridge(alpha=1.0)  # L2-Regularisierung\n",
    "dt = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "xgb = XGBRegressor(n_estimators=200, max_depth=5, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': lr,\n",
    "    'Lasso Regression': lasso,\n",
    "    'Ridge Regression': ridge,\n",
    "    'Decision Tree': dt,\n",
    "    'Random Forest': rf,\n",
    "    'Gradient Boosting': gb,\n",
    "    'XGBoost': xgb,\n",
    "}\n",
    "\n",
    "# Erstelle den Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('Linear Regression', lr),\n",
    "    ('Lasso Regression', lasso),\n",
    "    ('Ridge Regression', ridge),\n",
    "    ('Decision Tree', dt),\n",
    "    ('Random Forest', rf),\n",
    "    ('Gradient Boosting', gb),\n",
    "    ('XGBoost', xgb)\n",
    "])\n",
    "\n",
    "# Leave-One-Out-Cross-Validation (LOOCV)\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Ergebnisse speichern\n",
    "results = {model_name: {'mse': [], 'mae': [], 'y_true': [], 'y_pred': []} for model_name in models.keys()}\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        \n",
    "        results[model_name]['mse'].append(mse)\n",
    "        results[model_name]['mae'].append(mae)\n",
    "        results[model_name]['y_true'].append(y_test.values[0])\n",
    "        results[model_name]['y_pred'].append(predictions[0])\n",
    "\n",
    "# Durchschnittliche Fehlermaße über die LOOCV-Folds berechnen\n",
    "for model_name in models.keys():\n",
    "    avg_mse = np.mean(results[model_name]['mse'])\n",
    "    avg_mae = np.mean(results[model_name]['mae'])\n",
    "    overall_r2 = r2_score(results[model_name]['y_true'], results[model_name]['y_pred'])\n",
    "    \n",
    "    print(f\"{model_name} - Mean Absolute Error: {avg_mae}\")\n",
    "    print(f\"{model_name} - Mean Squared Error: {avg_mse}\")\n",
    "    print(f\"{model_name} - R^2: {overall_r2}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
